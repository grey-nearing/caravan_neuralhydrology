# --- Experiment configurations --------------------------------------------------------------------

# experiment name, used as folder name
experiment_name: except_us_0

run_dir: runs/country_runs

# files to specify training, validation and test basins (relative to code root or absolute path)
train_basin_file: basin_lists/countries/caravan_except_us_basins.txt
validation_basin_file: basin_lists/caravan_basins.txt
test_basin_file: basin_lists/caravan_basins.txt

# training, validation and test time periods (format = 'dd/mm/yyyy')
per_basin_train_periods_file: basin_lists/train_periods.pkl
per_basin_test_periods_file: basin_lists/test_periods.pkl
per_basin_validation_periods_file: basin_lists/test_periods.pkl

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu or None]
device: cuda:0

# --- Validation configuration ---------------------------------------------------------------------

# specify after how many epochs to perform validation
validate_every: 50

# specify how many random basins to use for validation
validate_n_random_basins: -1

# specify which metrics to calculate during validation (see neuralhydrology.evaluation.metrics)
# this can either be a list or a dictionary. If a dictionary is used, the inner keys must match the name of the
# target_variable specified below. Using dicts allows for different metrics per target variable.
metrics:
- NSE
- MSE
- RMSE
- KGE
- Alpha-NSE
- Pearson-r
- Beta-NSE
- FHV
- FMS
- FLV
- Peak-Timing

# --- Model configuration --------------------------------------------------------------------------

# base model type [lstm, ealstm, cudalstm, embcudalstm, mtslstm]
# (has to match the if statement in modelzoo/__init__.py)
model: cudalstm

# prediction head [regression]. Define the head specific parameters below
head: regression

# ----> Regression settings <----
output_activation: linear

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 256

# Initial bias value of the forget gate
initial_forget_bias: 3

# Dropout applied to the output of the LSTM
output_dropout: 0.4

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam]
optimizer: Adam

# specify loss [MSE, NSE, RMSE]
loss: NSE

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
    0: 1e-2
    5: 5e-3
    10: 1e-3

# Mini-batch size
batch_size: 1024

# Number of training epochs
epochs: 15

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1

# Defines which time steps are used to calculate the loss. Can't be larger than seq_length.
# If use_frequencies is used, this needs to be a dict mapping each frequency to a predict_last_n-value, else an int.
predict_last_n: 1

# Length of the input sequence
# If use_frequencies is used, this needs to be a dict mapping each frequency to a seq_length, else an int.
seq_length: 365

# Number of parallel workers used in the data pipeline
num_workers: 2

# Log the training loss every n steps
log_interval: 5

# If true, writes logging results into tensorboard file
log_tensorboard: True

# If a value and greater than 0, logs n random basins as figures during validation
log_n_figures: 0

# Save model weights every n epochs
save_weights_every: 1

# Discard all data from any basins with fewer than this many samples.
min_samples_per_basin: 365

# --- Data configurations --------------------------------------------------------------------------

# which data set to use [camels_us, camels_gb, global, hourly_camels_us]
dataset: caravan

# Path to data set root
data_dir: data

# Forcing product [daymet, maurer, maurer_extended, nldas, nldas_extended, nldas_hourly]
# can be either a list of forcings or a single forcing product
forcings:
- caravan

dynamic_inputs:
 - snow_depth_water_equivalent_mean
 - surface_net_solar_radiation_mean
 - surface_net_thermal_radiation_mean
 - surface_pressure_mean
 - temperature_2m_mean
 - dewpoint_temperature_2m_mean
 - u_component_of_wind_10m_mean
 - v_component_of_wind_10m_mean
 - volumetric_soil_water_layer_1_mean
 - volumetric_soil_water_layer_2_mean
 - volumetric_soil_water_layer_3_mean
 - volumetric_soil_water_layer_4_mean
 - snow_depth_water_equivalent_min
#  - surface_net_solar_radiation_min
 - surface_net_thermal_radiation_min
 - surface_pressure_min
 - temperature_2m_min
 - dewpoint_temperature_2m_min
 - u_component_of_wind_10m_min
 - v_component_of_wind_10m_min
 - volumetric_soil_water_layer_1_min
 - volumetric_soil_water_layer_2_min
 - volumetric_soil_water_layer_3_min
 - volumetric_soil_water_layer_4_min
 - snow_depth_water_equivalent_max
 - surface_net_solar_radiation_max
 - surface_net_thermal_radiation_max
 - surface_pressure_max
 - temperature_2m_max
 - dewpoint_temperature_2m_max
 - u_component_of_wind_10m_max
 - v_component_of_wind_10m_max
 - volumetric_soil_water_layer_1_max
 - volumetric_soil_water_layer_2_max
 - volumetric_soil_water_layer_3_max
 - volumetric_soil_water_layer_4_max
 - total_precipitation_sum
 - potential_evaporation_sum

# which columns to use as target
target_variables:
 - streamflow

# clip negative predictions to zero for all variables listed below. Should be a list, even for single variables.
clip_targets_to_zero:
 - streamflow

# Static basin attributes.
static_attributes:
 - basin_area
 - p_mean
 - pet_mean
 - aridity
 - frac_snow
 - moisture_index
 - seasonality
 - high_prec_freq
 - high_prec_dur
 - low_prec_freq
 - low_prec_dur
 - snd_pc_sav
 - slt_pc_sav
 - cly_pc_sav
 - sgr_dk_sav
 - gdp_ud_sav
 - gla_pc_sse
 - crp_pc_sse
 - pac_pc_sse
 - kar_pc_sse
 - inu_pc_slt
 - prm_pc_sse
 - pst_pc_sse
 - ire_pc_sse
 - for_pc_sse
 - urb_pc_sse
 - lka_pc_sse
# - hdi_ix_sav
 - gdp_ud_ssu
 - ppd_pk_sav
 - rdd_mk_sav
 - ele_mt_smx
 - ele_mt_smn
 - ele_mt_sav
 - gwt_cm_sav
 - slp_dg_sav
#  - hft_ix_s09
#  - hft_ix_s93
#  - dis_m3_pyr
#  - dis_m3_pmx
#  - dis_m3_pmn
#  - riv_tc_usu
#  - fec_cl_smj
#  - soc_th_sav
#  - tbi_cl_smj
#  - nli_ix_sav
#  - rev_mc_usu
#  - dor_pc_pva
#  - ero_kh_sav
#  - ari_ix_sav
#  - tec_cl_smj
#  - ria_ha_usu
#  - lkv_mc_usu
#  - fmh_cl_smj
#  - inu_pc_smn
#  - pnv_cl_smj
#  - pop_ct_ssu
#  - run_mm_syr
#  - inu_pc_smx
#  - clz_cl_smj
#  - lit_cl_smj
#  - cls_cl_smj
#  - wet_cl_smj
#  - wet_pc_sg1
#  - wet_pc_sg2
#  - wet_pc_s06
# # - wet_pc_s08
#  - wet_pc_s09
#  - wet_pc_s02
#  - wet_pc_s03
#  - wet_pc_s01
#  - wet_pc_s07
#  - wet_pc_s04
#  - wet_pc_s05
#  - glc_cl_smj
#  - glc_pc_s01
#  - glc_pc_s02
#  - glc_pc_s03
#  - glc_pc_s04
#  - glc_pc_s06
#  - glc_pc_s07
#  - glc_pc_s08
#  - glc_pc_s09
#  - glc_pc_s10
#  - glc_pc_s11
#  - glc_pc_s12
#  - glc_pc_s13
#  - glc_pc_s14
#  - glc_pc_s15
#  - glc_pc_s16
#  - glc_pc_s17
#  - glc_pc_s18
#  - glc_pc_s19
#  - glc_pc_s20
#  - glc_pc_s21
#  - glc_pc_s22
 - swc_pc_syr
#  - swc_pc_s01
#  - swc_pc_s02
#  - swc_pc_s03
#  - swc_pc_s04
#  - swc_pc_s05
#  - swc_pc_s06
#  - swc_pc_s07
#  - swc_pc_s08
#  - swc_pc_s09
#  - swc_pc_s10
#  - swc_pc_s11
#  - swc_pc_s12
 - tmp_dc_syr
 - tmp_dc_smn
 - tmp_dc_smx
#  - tmp_dc_s07
#  - tmp_dc_s08
#  - tmp_dc_s05
#  - tmp_dc_s06
#  - tmp_dc_s09
#  - tmp_dc_s10
#  - tmp_dc_s11
#  - tmp_dc_s12
#  - tmp_dc_s03
#  - tmp_dc_s04
#  - tmp_dc_s01
#  - tmp_dc_s02
 - aet_mm_syr
#  - aet_mm_s01
#  - aet_mm_s02
#  - aet_mm_s03
#  - aet_mm_s04
#  - aet_mm_s05
#  - aet_mm_s06
#  - aet_mm_s07
#  - aet_mm_s08
#  - aet_mm_s09
#  - aet_mm_s10
#  - aet_mm_s11
#  - aet_mm_s12
#  - pnv_pc_s01
#  - pnv_pc_s02
#  - pnv_pc_s03
#  - pnv_pc_s04
#  - pnv_pc_s05
#  - pnv_pc_s06
#  - pnv_pc_s07
#  - pnv_pc_s08
#  - pnv_pc_s09
#  - pnv_pc_s10
#  - pnv_pc_s11
#  - pnv_pc_s12
#  - pnv_pc_s13
#  - pnv_pc_s14
#  - pnv_pc_s15
 - pet_mm_syr
#  - pet_mm_s01
#  - pet_mm_s02
#  - pet_mm_s03
#  - pet_mm_s04
#  - pet_mm_s05
#  - pet_mm_s06
#  - pet_mm_s07
#  - pet_mm_s08
#  - pet_mm_s09
#  - pet_mm_s10
#  - pet_mm_s11
#  - pet_mm_s12
 - pre_mm_syr
#  - pre_mm_s01
#  - pre_mm_s02
#  - pre_mm_s03
#  - pre_mm_s04
#  - pre_mm_s05
#  - pre_mm_s06
#  - pre_mm_s07
#  - pre_mm_s08
#  - pre_mm_s09
#  - pre_mm_s10
#  - pre_mm_s11
#  - pre_mm_s12
 - snw_pc_syr
 - snw_pc_smx
#  - snw_pc_s01
#  - snw_pc_s02
#  - snw_pc_s03
#  - snw_pc_s04
#  - snw_pc_s05
#  - snw_pc_s06
#  - snw_pc_s07
#  - snw_pc_s08
#  - snw_pc_s09
#  - snw_pc_s10
#  - snw_pc_s11
#  - snw_pc_s12
 - cmi_ix_syr
#  - cmi_ix_s01
#  - cmi_ix_s02
#  - cmi_ix_s03
#  - cmi_ix_s04
#  - cmi_ix_s05
#  - cmi_ix_s06
#  - cmi_ix_s07
#  - cmi_ix_s08
#  - cmi_ix_s09
#  - cmi_ix_s10
#  - cmi_ix_s11
#  - cmi_ix_s12

# Path to pickle file(s) containing additional data. Each pickle file must contain a dictionary
# with one key for each basin and the value is a time indexed data frame, where each column is a 
# feature.
# Convention: If a column is used as static input, the value to use for specific sample should be in
# same row (datetime) as the target discharge value.
additional_feature_files:

# columns of the data frame to use as (additional) "static" inputs for each sample. Must be present in
# the above linked additional feature files. These values will be used as static inputs, but they can evolve over time.
# Leave empty to not use any.
evolving_attributes:

# whether to use basin id one hot encoding as (additional) static input
use_basin_id_encoding: False
